{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Collection Notebook\n",
    "\n",
    "This notebook collects academic papers from Semantic Scholar and arXiv based on research areas defined in `config.yaml`.\n",
    "\n",
    "## Workflow\n",
    "1. Load configuration\n",
    "2. Query APIs for each research area\n",
    "3. Deduplicate and merge results\n",
    "4. Save to data/raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from utils.paper_collector import PaperCollector\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "research_areas = config['research_areas']\n",
    "\n",
    "print(f\"üìö Research Areas: {len(research_areas)}\")\n",
    "for area_name, area_config in research_areas.items():\n",
    "    print(f\"  - {area_name}: {len(area_config['keywords'])} keywords, max {area_config['max_papers']} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "api_key = os.getenv('SEMANTIC_SCHOLAR_API_KEY')\n",
    "collector = PaperCollector(cache_dir='../cache', api_key=api_key)\n",
    "\n",
    "print(\"‚úÖ PaperCollector initialized\")\n",
    "if api_key:\n",
    "    print(\"   Using Semantic Scholar API key (higher rate limits)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  No API key - using default rate limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect papers for each research area\n",
    "all_papers_by_area = {}\n",
    "\n",
    "for area_name, area_config in tqdm(research_areas.items(), desc=\"Research Areas\"):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìñ Collecting papers for: {area_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    area_papers = []\n",
    "    keywords = area_config['keywords']\n",
    "    max_papers = area_config['max_papers']\n",
    "    \n",
    "    papers_per_keyword = max_papers // len(keywords)\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        print(f\"\\nüîç Searching: '{keyword}'\")\n",
    "        \n",
    "        # Search Semantic Scholar\n",
    "        try:\n",
    "            papers = collector.search_semantic_scholar(keyword, limit=papers_per_keyword)\n",
    "            area_papers.extend(papers)\n",
    "            print(f\"   Found {len(papers)} papers from Semantic Scholar\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching Semantic Scholar: {e}\")\n",
    "        \n",
    "        # Optional: Also search arXiv\n",
    "        # try:\n",
    "        #     arxiv_papers = collector.search_arxiv(keyword, max_results=papers_per_keyword//2)\n",
    "        #     area_papers.extend(arxiv_papers)\n",
    "        #     print(f\"   Found {len(arxiv_papers)} papers from arXiv\")\n",
    "        # except Exception as e:\n",
    "        #     logger.error(f\"Error searching arXiv: {e}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_papers = {}\n",
    "    for paper in area_papers:\n",
    "        paper_id = paper.get('paperId', paper.get('id'))\n",
    "        if paper_id and paper_id not in unique_papers:\n",
    "            unique_papers[paper_id] = paper\n",
    "    \n",
    "    area_papers = list(unique_papers.values())\n",
    "    all_papers_by_area[area_name] = area_papers\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total unique papers for {area_name}: {len(area_papers)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ Collection complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "total_papers = sum(len(papers) for papers in all_papers_by_area.values())\n",
    "\n",
    "print(\"üìä Collection Summary:\")\n",
    "print(f\"\\nTotal papers collected: {total_papers}\")\n",
    "print(\"\\nBy research area:\")\n",
    "\n",
    "for area_name, papers in all_papers_by_area.items():\n",
    "    print(f\"  {area_name:20s}: {len(papers):4d} papers\")\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "all_papers_flat = []\n",
    "for area_name, papers in all_papers_by_area.items():\n",
    "    for paper in papers:\n",
    "        paper_copy = paper.copy()\n",
    "        paper_copy['research_area'] = area_name\n",
    "        all_papers_flat.append(paper_copy)\n",
    "\n",
    "df = pd.DataFrame(all_papers_flat)\n",
    "\n",
    "print(f\"\\nüìà DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "if 'year' in df.columns:\n",
    "    print(f\"\\nYear range: {df['year'].min()} - {df['year'].max()}\")\n",
    "\n",
    "if 'citationCount' in df.columns:\n",
    "    print(f\"\\nCitation statistics:\")\n",
    "    print(f\"  Mean: {df['citationCount'].mean():.1f}\")\n",
    "    print(f\"  Median: {df['citationCount'].median():.1f}\")\n",
    "    print(f\"  Max: {df['citationCount'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path('../data/raw')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save by area\n",
    "output_file = output_dir / 'papers_by_area.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_papers_by_area, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved papers by area to: {output_file}\")\n",
    "\n",
    "# Save flat list\n",
    "output_file = output_dir / 'papers_metadata.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_papers_flat, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved all papers metadata to: {output_file}\")\n",
    "\n",
    "# Save DataFrame as CSV\n",
    "output_file = output_dir / 'papers_metadata.csv'\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Saved DataFrame to: {output_file}\")\n",
    "\n",
    "print(\"\\nüéâ All data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview top papers by citations\n",
    "if 'citationCount' in df.columns and 'title' in df.columns:\n",
    "    top_papers = df.nlargest(10, 'citationCount')[['title', 'year', 'citationCount', 'research_area']]\n",
    "    print(\"\\nüèÜ Top 10 Most Cited Papers:\")\n",
    "    print(\"\\n\" + top_papers.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}